# warehouse

This service allows data warehousing and workfow orchestration.

> Workflow service takes care of submission and monitoring of the actual applications on configured VM through ssh as well as staging input files between each tasks across heterogeneous computing resources through sftp. Workflow service is configured to run applications on optimal, and currently available computing resources that user has access to. 

> We envision that we could use Azure VM(with docker installed on it) or the Azure container service as one of our computing resources where our workflow service could execute our applications. Any data generated by such applications is stored locally on each computing resources, and it is considered temporary; the data will be purged within a few days of task completion. For Azure, we will need to make sure so that *most* tasks within a single workflow will be executed on Azure to minimize the cost incurred by data transfer between our other computing resources (assuming Azure will charge for sftp transfer).  

> Another service called "warehouse" service will take care of long term curation of data generated by workflow service (which includes user uploaded data - which is technically a part of workflow for our system). User will given a choice whether to archive the content from their workflow permanently. We expect the size of this archive will eventually grow to the size of around 100TB. Data can either be public, or private. If private, only the project members can download / process data from this archive. When a user requests to archive the data, warehouse service will tar-gz the workflow directories (including output from all tasks) from various computing resources and transfer them to our permanent storage locations along with storing some metadata in warehouse service's database. 

> We envision Azure Cool Blob Storage could be one of our primary storage systems for this archive. For workflows generated on Azure VM, we will use SMB mount to directly transfer data to Azure Blob Storage. For workflows that exists outside Azure, we will use Blog Service REST API to upload the data. Warehouse(-mover) service also takes care of staging the data out of archive and making it available for further processing by workflow service.  

It has following components

* Archive (storage systems that items can be physically stored, and database to keep track of where items are)
* Data type (list of various data type tree)
* Application registry (list of various applications (github URL/tags, and reference to data types for all input / output))
* 

## Archiving

This service allows users to

* Search items stored in the warehouse that they have access to (via projects)
* Copy wf/tasks in and out of warehouse (warehouse issues temporary access token which can then be used with cloudstorage-service (or use sca-data?) to transfer data to/from it as part of wf/tasks) * This doesn't allow us to take advantage of s3/bucket mounting.. I need different approach.

## Data Type Transition
