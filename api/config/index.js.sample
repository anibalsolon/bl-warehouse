const fs = require('fs');
const winston = require('winston');
const timeout = require('callback-timeout');
const child_process = require('child_process');

const mkdirp = require('mkdirp'); //for dc2
const pkgcloud = require('pkgcloud');
const path = require('path');
const archiver = require('archiver');
const ssh2 = require('ssh2');

exports.mongodb = "mongodb://localhost/warehouse";
exports.neo4j = "http://neo4j:whatever@localhost:7474";

//used to post/poll health status from various services
exports.redis = {
    server: "localhost",
    //port: 6379,
}

exports.wf = {
    api: "https://dev1.soichi.us/api/wf",
}

exports.auth = {
    api: "https://dev1.soichi.us/api/auth",
    
    //jwt used to request auth service to issue new user jwt to be used to submit tasks for the user
    //~/git/auth/bin/auth.js issue --scopes '{ "sca": ["admin"] }' --sub 'warehouse'
    jwt: fs.readFileSync(__dirname+'/auth.jwt', 'ascii').trim(),
}

exports.warehouse = {
    //used by rule handler to submit dataset download request
    api: "https://dev1.soichi.us/api/warehouse",
}

//for archive service
exports.archive = {
    //remporary path used to store downloaded datasets before shipping to hsi
    tmp: "/mnt/scratch/hayashis/archive-tmp",
}

//for event handler
exports.event = {
    amqp: {
        url: "amqp://warehouse:gowarehouse123@localhost:5672/brainlife"
    },
}

//for rule handler
exports.rule = {
    max_task_per_rule: 30, //limit number of concurrently running tasks submission
}

exports.express = {
    port: 12501,
    //public key used to validate jwt token
    pubkey: fs.readFileSync('/home/hayashis/git/auth/api/config/auth.pub'),
}

///////////////////////////////////////////////////////////////////////////////////////////////////
//
//  storage system where we can archive data
exports.storage_systems = {};

//pick default storage
exports.storage_default = function() {
    return "jetstream";
}

//
// jetstream
// //https://github.com/pkgcloud/pkgcloud/blob/master/examples/storage/rackspace.js
//
const js_config = require(__dirname+'/jetstream');
const js_storage = pkgcloud.storage.createClient(js_config);
exports.storage_systems.jetstream = {
    test: cb=>{
        js_storage.getContainers((err,containers)=>{
            if(err) return cb(err);
            cb();
        }); 
    },
    stat: (dataset, cb)=>{
        var contname = dataset.project.toString();
        var filename = dataset._id+".tar.gz"; //old ones were using .tar.gz

        if(dataset.storage_config && dataset.storage_config.filename) {
            contname = dataset.storage_config.contname;
            filename = dataset.storage_config.filename;
        }

        js_storage.getFile(contname, filename, (err,_stats)=>{
            if(err) return cb(err);
            cb(null, {
                size: _stats.size,
                mtime: _stats.lastModified,
            });
        });
    },
    upload: (dataset, cb)=>{
        var contname = dataset.project.toString();
        var filename = dataset._id+'.tar';
        console.log("uploading dataset", contname, filename);
    
        //make sure container exists (if it exists, it's noop)
        js_storage.createContainer({
            name: contname,
            metadata: {
                //TODO..project_name?
            },
        }, (err, container)=> {
            if(err) return cb(err);
            dataset.storage_config = { contname, filename };
            var stream = js_storage.upload({container, remote: filename});
            cb(null, stream);
        });
    },
    download: (dataset, cb)=>{
        var contname = dataset.project.toString();
        var filename = dataset._id+".tar.gz"; //old ones were using .tar.gz

        if(dataset.storage_config && dataset.storage_config.filename) {
            contname = dataset.storage_config.contname;
            filename = dataset.storage_config.filename;
        }
        var stream = js_storage.download({container: contname, remote: filename});
        cb(null, stream, filename);
    },
}

function connect_dc(cb) {
    var conn = new ssh2.Client();
    conn.on('ready', ()=>{
        cb(null, conn); 
    });
    try {
        conn.connect({
            username: "hayashis",
            host: "dataxfer2.bigred2.uits.iu.edu",
            privateKey: fs.readFileSync(__dirname+'/brlife.id_rsa'),
        });
    } catch(err) {
        cb(err);
    }
}

exports.storage_systems.dc2 = {
    /*
    //return archive_stream to pipe data to
    test: cb=>{
        fs.stat(dc2_path, timeout((err,stats)=>{
            if(err) return cb(err);
            if(!stats.isDirectory()) return cb(new Error("datasets directory is not a directory")); 
            cb();
        }, 2000, 'failed to stat '+dc2_path)); 
    }, 
    stat: (dataset, cb)=>{
        var dir = dc2_path+dataset.project.toString();
        var filename = dataset._id+".tar.gz"; //old ones were using .tar.gz
        if(dataset.storage_config && dataset.storage_config.filename) filename = dataset.storage_config.filename;
        fs.stat(dir+"/"+filename, cb);
    },
    upload: (dataset, cb)=>{
        var dir = dc2_path+dataset.project.toString();
        var filename = dataset._id+".tar"; 
        dataset.storage_config = { dir, filename }
        mkdirp(dir, (err)=>{
            if(err) return cb(err);
            cb(null, fs.createWriteStream(dir+'/'+filename));
        });
    },
    download: (dataset, cb)=>{
        var dir = dc2_path+dataset.project.toString();
        var filename = dataset._id+".tar.gz"; //old ones were using .tar.gz
        if(dataset.storage_config && dataset.storage_config.filename) filename = dataset.storage_config.filename;
        var stream = fs.createReadStream(dir+"/"+filename);
        cb(null, stream, filename);
    },
    */
    test: cb=>{
        //TODO - I should do more checking?
        connect_dc((err, conn)=>{
            conn.end();
            if(err) return cb(err);
            cb();
        });
    }, 
    stat: (dataset, cb)=>{
        //TODO..
        cb();
    },
    upload: (dataset, cb)=>{
        cb("no upload to dcwan/hcp");
    },
    download: (dataset, cb)=>{

        connect_dc((err, conn)=>{
            if(err) return cb(err);
             conn.sftp((err, sftp)=>{
                if (err) {
                    conn.end();
                    return cb(err);
                }
                var path = "/N/dc2/projects/brainlife/dev1-warehouse/datasets/"+dataset.project+"/"+dataset._id+".tar.gz";
                var stream = sftp.createReadStream(path);
                stream.on('close', code=>{
                    conn.end();
                });
                cb(null, stream, dataset._id+".tar.gz");
            });
        });
    },

} 

exports.storage_systems["dcwan/hcp"] = {
    /*
    const dcwan_hcp_path = "/mnt/dcwan/projects/brainlife/hcp/";
    test: cb=>{
        fs.stat(dcwan_hcp_path, timeout((err,stats)=>{
            if(err) return cb(err);
            if(!stats.isDirectory()) return cb(new Error("datasets directory is not a directory")); 
            cb();
        }, 2000, 'failed to stat '+dcwan_hcp_path)); 
    }, 
    stat: (dataset, cb)=>{
        //can't obtain stats for .tar.gz because we are creating it on the fly
        //but I can still check for the directory to exist and respond error
        var dir = dcwan_hcp_path+dataset.storage_config.subdir;
        fs.stat(dir, (err,stats)=>{
            //stats is for the directory.. so I can't return it
            cb(err, null); 
        });
    },
    upload: (dataset, cb)=>{
        //dataset.storage_config = { dir, filename }
        cb("no upload to dcwan/hcp");
    },
    download: (dataset, cb)=>{
        var dir = dcwan_hcp_path+dataset.storage_config.subdir;
        var child = child_process.spawn("tar", ["hc", "."], {cwd: dir});
        cb(null, child.stdout, dataset._id+".tar");
        child.stderr.on('data', function(data) {
            console.error(data.toString());
        });
        child.on('error', (err)=>{
            console.error(err);
        });
    },
    */
    test: cb=>{
        connect_dc((err, conn)=>{
            conn.end();
            if(err) return cb(err);
            cb();
        });
    }, 
    stat: (dataset, cb)=>{
        //TODO..
        cb();
    },
    upload: (dataset, cb)=>{
        cb("no upload to dcwan/hcp");
    },
    download: (dataset, cb)=>{
        connect_dc((err, conn)=>{
            if(err) return cb(err);

            if(dataset.storage_config.files) {
                 conn.sftp((err, sftp)=>{
                    if (err) {
                        conn.end();
                        return cb(err);
                    }

                    var archive = archiver('tar');
                    dataset.storage_config.files.forEach(file=>{
                        console.log("downloading", file.filepath, file.local);
                        var stream = sftp.createReadStream(file.filepath);
                        archive.append(stream, {name: file.local});
                    });
                    archive.on('finish', ()=>{
                        //console.log("closing archive");
                        conn.end();
                    });
                    cb(null, archive, dataset._id+".tar");
                    archive.finalize();
                });
            }

            if(dataset.storage_config.dirpath) {
                var local = dataset.storage_config.local;
                conn.exec("cd "+dataset.storage_config.dirpath+"/surf && tar hc * --transform 's|^|/"+local+"/|'", (err, stream)=>{
                    if(err) return next(err);
                    stream.on('close', code=>{
                        //console.log("done with tar stream - closing connection:",code)
                        conn.end();
                    });
                    cb(null, stream, dataset._id+".tar");
                });
            }
        });
    },
}

const nki_config = require(__dirname+'/nki');
const nki_storage = pkgcloud.storage.createClient(nki_config);
//some BREATHHOLD files are inaccessible
//https://www.nitrc.org/forum/forum.php?thread_id=8563&forum_id=1244
exports.storage_systems["nki"] = {
    test: cb=>{
        nki_storage.getContainer(nki_config.bucket, (err,container)=>{
            if(err) return cb(err);
            console.log("nki debug/test", container.files.length); //length should be 1000 (max)
            cb();
        }); 
    }, 
    stat: (dataset, cb)=>{
        //can't obtain stats for .tar.gz because we are creating it on the fly
        cb(null);
    },
    upload: (dataset, cb)=>{
        cb("read only");
    },
    download: (dataset, cb)=>{
        var archive = archiver('tar');
        dataset.storage_config.files.forEach(file=>{
            console.log("downloading", file.s3, "from", nki_config.bucket, file.local);
            var stream = nki_storage.download({container: nki_config.bucket, remote: file.s3});
            archive.append(stream, {name: file.local});
        });
        archive.finalize();
        cb(null, archive, dataset._id+".tar");
    },
}

exports.logger = {
    winston: {
        //hide headers which may contain jwt
        requestWhitelist: ['url', 'method', 'httpVersion', 'originalUrl', 'query'],
        transports: [
            //display all logs to console
            new winston.transports.Console({
                timestamp: function() {
                    var d = new Date();
                    if(process.env.NODE_APP_INSTANCE) {
                        return process.env.NODE_APP_INSTANCE + "> "+ d.toString();
                    } else {
                        return d.toString();
                    }
                },
                level: 'debug',
                colorize: true
            }),
        ]
    },
}

