'use strict';

const fs = require('fs');
const { createLogger, format, transports } = require('winston');
const { combine, timestamp, label, prettyPrint } = format;

const child_process = require('child_process');

const mkdirp = require('mkdirp'); //for dc2
const pkgcloud = require('pkgcloud');
const path = require('path');
const archiver = require('archiver');
const ssh2 = require('ssh2');
const request = require('request');
const zlib = require('zlib');
const async = require('async');

exports.mongodb = "mongodb://localhost/warehouse";
exports.mongoose_debug = true;

exports.debug = true; 

//used to post/poll health status from various services
exports.redis = {
    server: "localhost",
    //port: 6379,
}

exports.github = {
    //client_id: "6a581b698021c7f38c52",
    //client_secret: fs.readFileSync(__dirname+"/github.client.secret", "ascii").trim(),
    access_token: fs.readFileSync(__dirname+"/github.access_token", "ascii").trim(),
}

//admin+client scope oauth2 token used to invite users
//once you create oauth app, you will need to re-authorize it with admin+client (it gave with just admin scope)
//https://slack.com/oauth/authorize?&client_id=(your client id)&team=(teamId)&install_redirect=install-on-team&scope=admin+client
//more information can be found here https://github.com/outsideris/slack-invite-automation
/*
exports.slack = {
    token: fs.readFileSync(__dirname+"/slack.token", "ascii").trim(),
}
*/

exports.amaretti = {
    api: "https://dev1.soichi.us/api/amaretti",
}
exports.wf = exports.amaretti; //deprecated (use amaretti)

exports.profile = {
    api: "https://dev1.soichi.us/api/profile",
}

exports.auth = {
    api: "https://dev1.soichi.us/api/auth",
}

exports.warehouse = {
    //used by rule handler to submit dataset download request
    api: "https://dev1.soichi.us/api/warehouse",

    //base url
    url: "https://localhost.brainlife.io", //to test datacite

    //used to issue warehouse token to allow dataset download
    public_key: fs.readFileSync(__dirname+'/warehouse.pub'),
    private_key: fs.readFileSync(__dirname+'/warehouse.key'),

    //place to store rule logs 
    rule_logdir: "/tmp",

    //jwt used to access other services
    //submit task on amaretti
    //query gids from auth
    jwt: fs.readFileSync(__dirname+'/warehouse.jwt', 'ascii').trim(),
}

exports.metrics = {
    counts: {
        interval: 300, //emit every 5min
        path: "/usr/local/graphite.5min/warehouse.counts", 
        prefix: "dev.warehouse", 
    },

    service_prefix: "dev.amaretti.service",
    
    //graphite api (https://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api)
    //curl http://10.0.0.10/metrics/find?query=test.*
    //curl -o test.png http://10.0.0.10/render?target=prod.amaretti.service.*&height=800&width=600 
    //curl -o test.json "http://10.0.0.10/render?target=prod.amaretti.service.bcmcpher-app-networkmatrices&format=json&noNullPoints"
    api: "http://10.0.0.10",
}

/*
//for archive service
exports.archive = {
    //remporary path used to store downloaded datasets before shipping to hsi
    tmp: "/mnt/scratch/hayashis/archive-tmp",
}
*/

//for event handler
exports.event = {
    amqp: {
        url: "amqp://warehouse:gobrain@localhost:5672/brainlife"
    },
}

//for rule handler
exports.rule = {
    max_task_per_rule: 30, //limit number of concurrently running tasks submission
    nice: 10, //default nice value for stage/process tasks 
}

exports.express = {
    port: 12501,
    //public key used to validate jwt token
    pubkey: fs.readFileSync('/home/hayashis/git/auth/api/config/auth.pub'),
}

exports.datacite = {
    prefix: "10.0322/bldev.",  //test account
    username: "DATACITE.BL",
    password: fs.readFileSync(__dirname+'/datacite.password', {encoding: "ascii"}).trim(),
    api: "https://mds.test.datacite.org",
}

///////////////////////////////////////////////////////////////////////////////////////////////////
//
//  storage system where we can archive data
exports.storage_systems = {};

/*
//pick default storage (TODO - deprecate thsi and use archive.storage_default instead
exports.storage_default = function() {
    //return "jetstream";
    return "wrangler";
}
*/

exports.archive = {
    storage_default: "wrangler",  
    storage_config: {
        resource_id: "59ea931df82bb308c0197c3d", //wrangler
    },
    gid: 137, //warehouse group enabled to access storage service
}

//
// jetstream
// //https://github.com/pkgcloud/pkgcloud/blob/master/examples/storage/rackspace.js
//
const js_config = require(__dirname+'/jetstream');
const js_storage = pkgcloud.storage.createClient(js_config);
exports.storage_systems.jetstream = {
    need_backup: true,
    test: cb=>{
        js_storage.getContainers((err,containers)=>{
            if(err) return cb(err);
            cb();
        }); 
    },
    stat: (dataset, cb)=>{
        let contname = dataset.project.toString();
        let filename = dataset._id+".tar.gz"; //old ones were using .tar.gz

        if(dataset.storage_config && dataset.storage_config.filename) {
            contname = dataset.storage_config.contname;
            filename = dataset.storage_config.filename;
        }

        console.log(contname);
        console.log(filename);
        js_storage.getFile(contname, filename, (err,_stats)=>{
            if(err) return cb(err);
            console.dir(_stats);
            cb(null, {
                size: _stats.size,
                mtime: _stats.lastModified,
            });
        });
    },
    upload: (dataset, cb)=>{
        let contname = dataset.project.toString();
        let filename = dataset._id+'.tar';
        console.log("uploading dataset", contname, filename);

        //make sure container exists (if it exists, it's noop)
        js_storage.createContainer({
            name: contname,
            metadata: {
                //TODO..project_name?
            },
        }, (err, container)=> {
            if(err) return cb(err);
            let stream = js_storage.upload({container, remote: filename});
            let success = new Promise((resolve, reject)=>{
                stream.on('success', resolve);
                stream.on('error', reject);
            });
        
            dataset.storage_config = { contname, filename }; //anti-pattern to update dataset here..
            cb(null, stream, success);
        });
    },
    download: (dataset, cb)=>{
        let contname = dataset.project.toString();
        let filename = dataset._id+".tar.gz"; //old ones were using .tar.gz
        if(dataset.storage_config && dataset.storage_config.filename) {
            contname = dataset.storage_config.contname;
            filename = dataset.storage_config.filename;
        }
        let stream = js_storage.download({container: contname, remote: filename});
        cb(null, stream, filename);
    },
    remove: (dataset, cb)=>{
        let contname = dataset.project.toString();
        let filename = dataset._id+'.tar.gz'; //old one were using .tar.gz
        if(dataset.storage_config && dataset.storage_config.filename) {
            contname = dataset.storage_config.contname;
            filename = dataset.storage_config.filename;
        }
        console.log("removing", contname, filename);
        js_storage.removeFile(contname, filename, cb);
    }
}

function connect_dc(cb) {
    var conn = new ssh2.Client();
    conn.on('ready', ()=>{
        cb(null, conn); 
    });
    conn.on('error', err=>{
        cb(err);
    });
    try {
        console.log("connecting to dataxfer2");
        conn.connect({
            username: "hayashis",
            host: "dataxfer2.bigred2.uits.iu.edu",
            privateKey: fs.readFileSync(__dirname+'/brlife.id_rsa'),
            //passphrase: "somepass",  //what's the point?
        });
    } catch(err) {
        cb(err);
    }
}

//TODO - I should try connection queue agagin
function connect_wrangler(cb) {
    var conn = new ssh2.Client();
    conn.on('ready', function() {
        console.log("connected to wrangler - opening sftp stream also");
        conn.sftp((err, sftp)=>{
            if(err) return cb(err);
            if(cb) cb(null, conn, sftp);
            cb = null;
        });
    });
    conn.on('end', function() {
        console.log("wrangler connection ended");
    });
    conn.on('close', function() {
        console.log("wrangler connection closed");
    });
    conn.on('error', function(err) {
        console.error("wrangler connectionn error");
        //we want to return connection error to caller, but error could fire after ready event is called.
        //like timeout, or abnormal disconnect, etc..  need to prevent calling cb twice!
        if(cb) cb(err);
        cb = null;
    });

    conn.connect({
        //username: "hayashis",
        username: "brlife",
        //host: "wrangler.iu.xsede.org",
        host: "149.165.156.63",
        privateKey: fs.readFileSync(__dirname+'/brlife.id_rsa'),
        //passphrase: "somepass",  //what's the point?
        keepaliveInterval: 10*1000, //default 0 (disabled)
        //keepaliveCountMax: 30, //default 3 (https://github.com/mscdex/ssh2/issues/367)
    });
}

function get_wrangler_archive_path(dataset) {
    //let _path = "/home/04040/hayashis/data/dev-archive/";
    let _path = "/mnt/wrangler/dev-archive/";
    _path += dataset.project+"/";
    _path += dataset._id+".tar";
    console.log(_path);
    return _path;
}

exports.storage_systems.wrangler = {
    need_backup: true, //need_backup if it's our storage (not openneuro)
    test: cb=>{
        connect_wrangler((err, conn, sftp)=>{
            if(err) return cb(err);
            console.log("loading quota.info from wrangler homedir.. should be created by cron..");
            let stream = sftp.createReadStream("/mnt/wrangler/my-quota", {encoding: "ascii"});
            let content = "";
            stream.on('readable', function() {
                //could be called multiple times..
                let chunk;
                while (null !== (chunk = stream.read())) {
                    content += chunk.toString();
                }
            });
            stream.on('end', ()=>{
                conn.end();
                //parse quota content
                let lines = content.split("\n");
                if(lines[0] != "Disk quotas for usr hayashis (uid 833400):") {
                    return cb("quota info looks odd.. "+content);
                }
                let stats = lines[3].trim().split(/[ ]+/);
                let disk_used = parseInt(stats[0]);
                let disk_quota = parseInt(stats[2]);
                let files_used = parseInt(stats[4]);
                let files_quota = parseInt(stats[6]);
                if(disk_used / disk_quota > 0.85) return cb("disk >85% used");
                if(files_used / files_quota > 0.85) return cb("files >85% used");
                cb();
            });
        });
    }, 

    stat: (dataset, cb)=>{
        connect_wrangler((err, conn, sftp)=>{
            if(err) return cb(err);
            sftp.stat(get_wrangler_archive_path(dataset), (err,stat)=>{
                conn.end();
                cb(err, stat);
            });
        });
    },

    download: (dataset, cb)=>{
        connect_wrangler((err, conn, sftp)=>{
            if (err) {
                conn.end();
                return cb(err);
            }
            let path = get_wrangler_archive_path(dataset);
            let filename = path.split("/").pop();
            console.debug(["loading...", path, filename]);
            conn.exec("cat "+path, (err, stream)=>{
                stream.on('error', err=>{
                    console.error("stream failed but train has already left the station", err);
                });
                stream.on('close', code=>{
                    conn.end();
                });
                //I believe listening to stream is required for close event to fire..?
                stream.stderr.on('data', data=>{
                    console.error(data.toString());
                });
                cb(err, stream, filename);
            });
        });
    },

    remove: (dataset, cb)=>{
        connect_wrangler((err, conn, sftp)=>{
            console.log("got wrangler streams for remove");
            if(err) return cb(err);
            let _path = get_wrangler_archive_path(dataset);
            conn.exec("rm -f "+_path, (err, stream)=>{
                if(err) {
                    conn.end();
                    return cb(err);
                }
                stream.on('error', err=>{
                    console.error(err);
                });
                stream.on('close', (code, signal)=>{
                    conn.end(); 
                    if(code == 0) return cb(null);
                    else cb("rm failed with code:"+code);
                });
                stream.on('data', data=>{
                    console.log(data.toString());
                });
                stream.stderr.on('data', data=>{
                    console.error(data.toString());
                });
            });
        });
    }
} 

exports.storage_systems.project = {
    need_backup: true, //need_backup if it's our storage (not openneuro)
    test: cb=>{
        console.log("project/test - todo");
        cb();
    }, 

    stat: (dataset, cb)=>{
        cb();
    },

    download: (dataset, cb)=>{
        let resource_id = dataset.storage_config.resource_id; 
        let path = dataset.project+"/"+dataset._id+".tar";
        let stream = request.get({
            url: exports.amaretti.api+"/resource/archive/download/"+resource_id+"/"+path, 
            headers: {Authorization: "Bearer "+ exports.warehouse.jwt}
        });
        cb(null, stream, dataset._id+".tar");
    },

    remove: (dataset, cb)=>{
        console.log("project/remove - todo");
    }
} 

function get_copysource_dataset(dataset) {
    return Object.assign({}, dataset, {
        project: dataset.storage_config.project,
        storage: dataset.storage_config.storage,
        storage_config: dataset.storage_config.storage_config,
        _id: dataset.storage_config.dataset_id,
    });
}
exports.storage_systems.copy = {
    need_backup: false,
    test: cb=>{
        cb();
    }, 

    stat: (dataset, cb)=>{
        let source_dataset = get_copysource_dataset(dataset);
        let system = exports.storage_systems[source_dataset.storage];
        system.stat(source_dataset, cb);
    },

    //TODO - with app-archive in place, I don't think we use this anymore
    upload: (dataset, cb)=>{
        cb("can't upload to copy target");
    },

    download: (dataset, cb)=>{
        let source_dataset = get_copysource_dataset(dataset);
        let system = exports.storage_systems[source_dataset.storage];
        system.download(source_dataset, cb);
    },
} 

exports.storage_systems.dc2 = {
    need_backup: false,
    test: cb=>{
        //TODO - I should do more checking?
        connect_dc((err, conn)=>{
            if(err) return cb(err);
            conn.end();
            cb();
        });
    }, 
    stat: (dataset, cb)=>{
        //TODO..
        cb();
    },
    upload: (dataset, cb)=>{
        cb("no upload to dc2");
    },
    download: (dataset, cb)=>{
        connect_dc((err, conn)=>{
            if(err) return cb(err);
             conn.sftp((err, sftp)=>{
                if (err) {
                    conn.end();
                    return cb(err);
                }
                var path = "/N/dc2/projects/brainlife/dev1-warehouse/datasets/"+dataset.project+"/"+dataset._id+".tar.gz";
                var stream = sftp.createReadStream(path);
                stream.on('error', err=>{
                    console.error("stream failed but train has already left the station", err);
                });
                stream.on('close', code=>{
                    conn.end();
                });
                cb(null, stream, dataset._id+".tar.gz");
            });
        });
    },
} 

exports.storage_systems["dcwan/hcp"] = {
    test: cb=>{
        connect_dc((err, conn)=>{
            if(err) return cb(err);
            conn.end();
            cb();
        });
    }, 
    stat: (dataset, cb)=>{
        //TODO..
        cb();
    },
    upload: (dataset, cb)=>{
        cb("no upload to dcwan/hcp");
    },
    download: (dataset, cb)=>{
        connect_dc((err, conn)=>{
            if(err) return cb(err);
            if(dataset.storage_config.files) {
                 conn.sftp((err, sftp)=>{
                    if (err) {
                        conn.end();
                        return cb(err);
                    }

                    //TODO - archiver seems to leak memory really bad.. 
                    //I can't really recreate the problem, but I think it happens if more data is fed to archiver
                    //than archiver can stream it out - using some kind of an internal buffer that never gets released
                    var archive = archiver('tar');
                    dataset.storage_config.files.forEach(file=>{
                        var stream = sftp.createReadStream(file.filepath);
                        stream.on('error', err=>{
                            console.error("stream failed but train has already left the station", file.filepath, err);
                        });
                        stream.on('end', ()=>{
                            console.log("stream ended", file.filepath);
                        });
                        console.log("archiver downloading", file.filepath, file.local);
                        archive.append(stream, {name: file.local});
                    });
                    archive.on('finish', ()=>{
                        console.log("archive finished");
                    });
                    archive.on('end', ()=>{
                        console.log("archive ended.. conn.end()");
                        conn.end();
                    });
                    archive.on('error', err=>{
                        console.log("archive error.......................");
                        console.error(err);
                    });
                    console.log("archive finalized");
                    archive.finalize();
                    cb(null, archive, dataset._id+".tar");
                });
            }

            if(dataset.storage_config.dirpath) {
                var local = dataset.storage_config.local;
                conn.exec("cd "+dataset.storage_config.dirpath+" && tar hc * --transform 's|^|/"+local+"/|'", (err, stream)=>{
                    if(err) return cb(err);
                    stream.on('close', code=>{
                        //console.log("done with tar stream - closing connection:",code)
                        conn.end();
                    });
                    cb(null, stream, dataset._id+".tar");
                });
            }
        });
    },
}

const nki_config = require(__dirname+'/nki');
const nki_storage = pkgcloud.storage.createClient(nki_config);
//some BREATHHOLD files are inaccessible
//https://www.nitrc.org/forum/forum.php?thread_id=8563&forum_id=1244
exports.storage_systems["nki"] = {
    test: cb=>{
        nki_storage.getContainer(nki_config.bucket, (err,container)=>{
            if(err) return cb(err);
            console.log("nki debug/test", container.files.length); //length should be 1000 (max)
            cb();
        }); 
    }, 
    stat: (dataset, cb)=>{
        //can't obtain stats for .tar.gz because we are creating it on the fly
        cb(null);
    },
    upload: (dataset, cb)=>{
        cb("read only");
    },
    download: (dataset, cb)=>{
        var archive = archiver('tar');
        dataset.storage_config.files.forEach(file=>{
            console.log("downloading", file.s3, "from", nki_config.bucket, file.local);
            var stream = nki_storage.download({container: nki_config.bucket, remote: file.s3});
            archive.append(stream, {name: file.local});
        });
        archive.finalize();
        cb(null, archive, dataset._id+".tar");
    },
}

/*
//TODO - deprecate this in favor of url
const openneuro_config = require(__dirname+'/openneuro');
const openneuro_storage = pkgcloud.storage.createClient(openneuro_config);
//I don't know how to stream directory structures like freesurfer..
exports.storage_systems["openneuro"] = {
    test: cb=>{
        openneuro_storage.getContainer(openneuro_config.bucket, (err,container)=>{
            if(err) return cb(err);
            console.log("openneuro debug/test", container.files.length); //length should be 1000 (max)
            cb();
        }); 
    }, 
    stat: (dataset, cb)=>{
        //can't obtain stats for .tar.gz because we are creating it on the fly
        cb(null);
    },
    upload: (dataset, cb)=>{
        cb("read only");
    },
    download: (dataset, cb)=>{
        var archive = archiver('tar');
        let gzip = zlib.createGzip();
        dataset.storage_config.files.forEach(file=>{
            console.log("downloading", file.s3, "from", openneuro_config.bucket, file.local);
            //downloading from s3 takes 30 seconds before the first byte arrives.
            //it's not archiver's fault or anything else.
            //https://github.com/pkgcloud/pkgcloud/issues/596
            var stream = openneuro_storage.download({container: openneuro_config.bucket, remote: file.s3});

            archive.append(stream, {name: file.local});
        });
        archive.finalize();
        cb(null, archive, dataset._id+".tar");
    },
}
*/

exports.storage_systems["url"] = {
    need_backup: false,
    test: cb=>{
        //TODO - maybe check to see if we have outbound connection?
        cb();
    }, 

    stat: (dataset, cb)=>{
        //no stats as .tar will be generated on the fly
        cb(null);
    },

    upload: (dataset, cb)=>{
        cb("read only");
    },

    download: (dataset, cb)=>{
        let archive = archiver('tar');
        let gzip = zlib.createGzip();
        dataset.storage_config.files.forEach(file=>{
            let stream = request(file.url);
            if(file.url.endsWith(".nii")) {
                //compress .nii with .nii.gz as *all* brainlife datatype uses .nii.gz for nifti
                console.log("passing", file.url, "through gzip stream");
                //archive.append(stream.pipe(gzip), {name: file.local});
                stream = stream.pipe(gzip);
            }
            archive.append(stream, {name: file.local});
        });
        archive.finalize();
        cb(null, archive, dataset._id+".tar");
    },
}

const datalad_root = "/mnt/datalad";
exports.storage_systems["datalad"] = {
    need_backup: false,
    test: cb=>{
        //TODO??
        cb();
    }, 

    stat: (dataset, cb)=>{
        //no stats as .tar will be generated on the fly
        cb(null);
    },

    upload: (dataset, cb)=>{
        cb("read only");
    },

    download: (dataset, cb)=>{
        //console.log("datalad getting");
        //run datalad get on each files
        async.each(dataset.storage_config.files, (file, next_file)=>{
            child_process.exec("datalad get "+file.src, {cwd: datalad_root}, (err, stdout, stderr)=>{
                if(err) return next_file(err);
                console.debug(stdout);
                console.error(stderr);
                next_file();
            });
        }, err=>{
            //create tar ball
            let archive = archiver('tar');
            let gzip = zlib.createGzip();
            dataset.storage_config.files.forEach(file=>{
                let stream = fs.createReadStream(datalad_root+"/"+file.src);
                if(file.src.endsWith(".nii")) {
                    //compress .nii with .nii.gz as *all* brainlife datatype uses .nii.gz for nifti
                    console.log("passing", file.src, "through gzip stream");
                    stream = stream.pipe(gzip);
                }
                archive.append(stream, {name: file.dest});
            });
            archive.finalize();
            cb(null, archive, dataset._id+".tar");
        });
    },
}


//config used to backup data from warehouse 
exports.sda = {
    ssh: {
        username: "hayashis",
        host: "sftp.sdarchive.iu.edu",
        privateKey: fs.readFileSync('/home/hayashis/.ssh/id_rsa').toString(),
        //debug: console.log,
    },
    basedir: "test",
}

exports.logger = {
    winston: {
        level: 'debug',
        format: combine(
            label({ label: 'warehouse-dev' }),
            timestamp(),
            format.colorize(),
            format.splat(),
            format.printf(info=>{
                return `${info.timestamp} [${info.label}] ${info.level}: ${info.message}`;
            }),
        ),
        
        //hide headers which may contain jwt
        requestWhitelist: ['url', 'method', 'httpVersion', 'originalUrl', 'query'],
        exceptionHandlers: [
            new transports.Console(),
        ],
        transports: [
            //display all logs to console
            new transports.Console({
                stderrLevels: ["error"], //error is sent to stdout by default..
            }),
        ]
    },
}

